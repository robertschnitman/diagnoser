---
title: 'schnitr'
author: 'Robert Schnitman'
date: 'November 14, 2017'
output:
  md_document:
    variant: markdown_github
---

```{r setup, include = FALSE}
setwd('~/MEGA/Personal/Scripts/R')
library(knitr)
data(mtcars)

fs <- list('diagnose.r', 'fitres.r', 'fitresdf.r',
        'glmdf.r', 'lmdf.r', 'wfreqdf.r')

lapply(fs, function(x) source(x))

```

# 1. Introduction

This repository hosts my family of R functions for statistical modeling, diagnostics, and data management. Many of them are inspired by tidyverse's broom library; but I was not quite satisfied with the mentioned libraries outputs (such as the lack of confidence intervals for OLS estimates). Others are simply "shortcuts" for standard procedures, such as residual analysis and word frequencies. More functions will be added as they come.

The following sections provide examples.

# 2. diagnose()

The function diagnose() provides an alternative for the plot(model.lm) approach. The Q-Q, Scale-Location, and Residuals-vs.-Leverage plots in the latter option can present difficulties in interpretations. For example, Cook's Distance ("Leverage") typically is not taught at the secondary and undergraduate level--when it is, teachers will forego explanation of the math and focus on the interpretation, leaving students in the dark. If the goal is to maximize student's understanding of diagnosing their results, one option is to replace the three previously mentioned graphs with histograms and an addition of another variable: residuals as a proportion of the fitted values (i.e. residuals ÷ fitted values).

```{r s2}
model.lm <- lm(data = mtcars, formula = mpg ~ wt + gear)

diagnose(model.lm)

```

# 3. fitres() & fitresdf()

The functions fitres() and fitresdf() will look similar to those who have used augment() from tidyverse's broom.

The former creates a matrix of the fitted values, residuals, and residuals as a proportion (percent) based on an OLS or GLM model. The latter *appends* these items as columns to a specified dataset (usually, the original dataset; but it can also be used for training & test datasets of the same size).

### fitres()
```{r s3-1}
model.lm <- lm(data = mtcars, formula = mpg ~ wt + gear)

head(fitres(model.lm))
```

### fitresdf()
```{r s3-2}
model.lm <- lm(data = mtcars, formula = mpg ~ wt + gear)

head(fitresdf(data = mtcars, model = model.lm))
```

# 4. lmdf() & glmdf()

The functions lmdf() and glmdf() has similar features to tidying model objects with broom--better variables in the output, but these two functions can only handle lm() and glm() objects.

The former presents OLS estimates with a margin of error and confidence intervals. The confidence level can be specified (90, 95, or 99) or left to the default value of 95 (representing 95% confidence). The latter function applies for GLM objects.

### lmdf()
```{r s4-1}
model.lm <- lm(data = mtcars, formula = mpg ~ wt + gear)

lmdf(model = model.lm, conf = 90)
lmdf(model = model.lm, conf = 95) # conf = 95 is the default value; can be omitted.
lmdf(model = model.lm, conf = 99)
```

### glmdf()
```{r s4-2}
model.glm <- glm(data = mtcars, formula = am ~ mpg + gear, family = binomial(link = 'logit'))

glmdf(model = model.glm, conf = 90)
glmdf(model = model.glm, conf = 95) # conf = 95 is the default value; can be omitted.
glmdf(model = model.glm, conf = 99)
```

# 5. clean_corpus() and wfreqdf() #

When I first learned about using the tm library, the process for converting raw text into a data frame of word frequencies was tedious: several tm_map()'s had to be applied to the corpus and transformations for the TDM-to-dataframe ordeal. As a result, I created two functions specifically for the overall procedure from raw text to data frame. It is especially useful for bar charts.

At this time, these functions "work" for English stop words. Other languages can be used for the stopwords argument, but perhaps they won't work as well as for English (for now!).

```{r s5, warning = FALSE}
# library(tm) assumed.
# wfreqdf() depends on clean_corpus().
# setting stopwords is based on the tm library. E.g. English --> stopwords = 'english'

head(wfreqdf(filename = 'letter.txt', stopwords = 'english'))

```

# 6. Conclusion and Future Work

I hope to improve upon these existing functions and create new ones that (1) minimize the programming tedium in statistical reporting and (2) assist people in diagnosing the validity of their results.

### In the future... ###
1. Functions similar to broom's glance() (perhaps with other model diagnostics and making "statistic" be clear that it is referring to the F-statistic).
2. ggplot2 version of diagnose().
3. Add VIF in lmdf() & glmdf().
4. For wfreqdf, test for Japanese stop words (my kanji is extremely weak).

*End of Document*